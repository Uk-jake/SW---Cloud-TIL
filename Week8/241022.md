# 오늘 배운 것
- **주요 개념**: Docker Swarm, Kubernetes
- **구체적인 내용**
    - Docker Swarm 초기설정
    - Docker Swarm 서비스 생성
    - Kubernetes 기초 개념

# 상세 학습 내용
## 리눅스 네트워크 구성(using Virtual Box)

### 1. VM 복제

- 기존에 사용하던 VM을 중지하고 [파일] 메뉴에서 [복제]를 선택
- MAC 주소 정책에서 모든 네트워크 어텝터의 새 MAC 주소 설정

### 2. IP 설정

- Wired Settings를 클릭해서 IP, SubnetMask, GateWay 설정
    
    ![image.png](10%2022(8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1)%20127e180c83b180d5aac8cbd2d32e7aa8/image.png)
    
1. swarm-manger : 192.168.200.200
2. swarm-worker1 : 192.168.200.201
3. swarm-worker3 : 192.168.200.202

### 3. DNS 설정

- /etc/hosts 파일 수정
    
    `sudo nano /etc/hosts`
    
1. swarm-manager
    
    ![image.png](10%2022(8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1)%20127e180c83b180d5aac8cbd2d32e7aa8/image%201.png)
    
2. swarm-worker1
    
    ![image.png](10%2022(8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1)%20127e180c83b180d5aac8cbd2d32e7aa8/image%202.png)
    
3. swarm-worker2
    
    ![image.png](10%2022(8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1)%20127e180c83b180d5aac8cbd2d32e7aa8/image%203.png)
    

## 다중 호스트 기반의 도커 스웜 모드 클러스터

### 1. 도커 스웜 모드

- 물리적 서버 클러스터를 통해서 컨테이너를 확장하기 위한 도커 고유의 플랫폼으로 여러 서버에 걸쳐 간단한 분산 워크로드를 구현할 수 있습니다.
- 여러 서버 클러스터에 컨테이너 화 된 애플리케이션 기반의 마이크로 서비스를 배포한 후, 다양한 런타임 환경에서 애플리케이션의 효율성과 가용성을 유지하도록 개발
- 도커 스웜 모드는 동일한 컨테이너를 공유하는 노드에서 애플리케이션을 원할하게 실행할 수 있도록 하는 컨테이너 **오케스트레이션** 도구
- 클러스터로 묶인 각 서버의 도커 엔진을 통해 하나의 서버처럼 풀링해서 스웜(여러 대의 서버를 묶어서 하나의 서버처럼 사용)을 형성
- 예전 버전에서는 별도의 스웜 엔진을 가지고 있었지만 지금은 도커 엔진으로 통합되어 기본 도커 오케스트레이션 도구 모델로 사용되기 시작

### 주요 기능

- 도커 엔진과 통합된 다중 서버 클러스터 환경: 도커 엔진에 포함된 도커 스웜 모드를 통해 별도의 오케스트레이션 도구를 설치하지 않아도 컨테이너 애플리케이션 서비스를 배포하고 관리할 수 있음
- 역할이 분리된 분산 설계
    - 다중 서버를 클러스터에 합류시키면 모든 도커 스웜 모드의 노드는 각각 다른 역할을 수행하게 됩니다.
        
        Manager Node
        
        Leader Node
        
        Worker Node
        
    - 단일 관리자 환경이라면 Manager Node 와 Worker Node로만 나누게 되지만 다중 매니저 모드로 구성하게 되면 매니저 중 하나를 Leader Node로 설정해서 수행
    - Manager Node는 클러스터의 관리 역할로 컨테이너 스케쥴링 서비스 및 상태 유지 등을 제공하고 작업자 노드는 컨테이너를 실행하는 역할만 수행
    - 쿠버네티스의 마스터 노드는 기본적으로 작업자 노드의 전체적인 관리만 수행하고 서비스 컨테이너는 수행하지 않지만(변경은 가능) 도커 스웜의 관리자 노드는 작업자 노드의 역할인 서비스 컨테이너도 수행할 수 있음
    - 관리 역할을 수행하는 노드의 부하를 고려해서 각 역할은 분리해 사용하는 것을 권장
    - 매니저 노드에 서비스 컨테이너를 수행하지 않도록 역할 분리를 수행하는 방법은 도커 서비스 생성 시 --constraint node.role!=manager 옵션을 사용하면 됩니다.
- 서비스 확장 과 원하는 상태 조정
    - 도커 스웜 모드에서 서비스 생성 시 안정적인 서비스를 위해 중복(복제 - replica)된 서비스 배포를 할 수 있고 초기 구성 후 스웜 관리자를 통해 애플리케이션 가용성에 영향을 주지 않고도 서비스 확장 및 축소를 수행할 수 있습니다
    - 서비스가 배포되면 매니저 노드를 통해서 지속적으로 모니터링을 수행
    - 사용자가 요청한 상태와 다르게 서비스 장애(노드 장애 및 서비스 실패)가 생길 경우 장애가 발생한 서비스를 대체할 복제본을 자동으로 생성해서 사용자의 요구를 지속하는데 이를 요구 상태 관리(desire state management)라고 합니다.
    - 오케스트레이션 도구들의 대부분의 목적은 요구 상태 관리
- 서비스 스케쥴링
    - 클러스터 내의 노드에 작업(task) 단위의 서비스 컨테이너를 배포하는 작업
    - 선택 전략
        
        모든 작업자 노드에 균등하게 할당하는 spread 전략
        
        작업자 노드의 자원 사용량을 고려하여 할당하는 binpack 전략
        
        임의의 노드에 할당하는 random 전략
        
    - swarm manage --strategy 옵션을 이용해서 변경 가능
    - 도커 스웜 모드는 고가용성 분산 알고리즘을 사용하는데 이는 생성되는 서비스의 복제본을 분산 배포하기 위해서 현재 복제본이 가장 적은 작업자 노드 중에서 이미 스케줄링된 다른 서비스 컨테이너가 가장 적은 작업자 노드를 우선 선택
- 로드 밸런싱
    - 도커 스웜 모드를 초기화하면 자동으로 생성되는 네트워크 드라이버 하나가 인그레스 네트워크(ingress network) 인데 인그레스 네트워크를 통해 서비스의 노드간 로드 밸런싱 과 외부에 서비스를 노출합니다.
    - 도커 스웜 모드는 서비스 컨테이너에 Published Port를 자동으로 할당하거나 수동으로 노출할 포트를 구성할 수 있고 서비스 컨테이너가 포트를 오픈하면 동시에 모든 노드에서 동일한 포트가 오픈되기 때문에 클러스터에 합류되어 있는 어떤 노드에 요청을 전달해도 실행 중인 서비스 컨테이너에 자동으로 전달됩니다.
- 서비스 검색 기능
    - 도커 스웜 모드는 서비스 검색을 위한 자체 DNS 서버를 통해서 서비스 검색 기능을 제공
    - 도커 스웜 모드의 매니저 노드는 클러스터에 합류된 모든 노드의 서비스에 고유한 DNS 이름을 할당하고 할당된 DNS 이름은 도커 스웜 모드에 내장된 DNS 서버를 통해 스웜 모드에서 실행 중인 모든 컨테이너를 조회하는 것이 가능한데 이를 service discovery
- 롤링 업데이트
    - 노드 단위로 점진적인 업데이트를 수행하는 것을 롤링 업데이트라고 합니다.
    - 롤링 업데이트는 각 작업자 노드에서 실행 중인 서비스 컨테이너를 노드 단위의 지연적 업데이트를 수행하는 것
    - 업데이트 지연 시간을 설정해서 하나의 작업자 노드에서 기존 컨테이너를 중지하고 새로운 서비스 컨테이너를 생성하고 성공하면 지연 시간만큼 기다린 후 다른 작업자 노드에서 동일한 작업을 수행
    - 새 버전의 서비스 컨테이너를 하나씩 늘려가면서 이전 버전의 서비스 컨테이너를 줄여나가는 방식
    - 업데이트 실패 시 중지 및 롤백 기능도 제공

## Docker Swarm 초기 설정

![image.png](10%2022(8%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1)%20127e180c83b180d5aac8cbd2d32e7aa8/image%204.png)

### 1. Manager 초기화

1. 초기화 명령
    - `docker swarm init --advertise-addr [아이피]`
    - `docker swarm init --advertise-addr 192.168.200.200`
    - docker swarm join --token SWMTKN-1-5foa5hg6ym28v173lmodpf2uod5gocfazx1t70c1d6t3huz094-48fofhgu7avd420a021qmiovr 192.168.201.200:2377
    - Swarm 모드가 되면 2477 포트와 7946포트 개방
        - `sudo netstat -nlp | grep dockerd`

### 2. Worker 초기화

1. 작업자 노드 연결
    - docker swarm join —token [토근값] [managerIP]:2377
    - docker swarm join --token SWMTKN-1-1h32vftyx4s3vkkl1bunxe8ungw7g4j0hueo2tlym1i0j8flyb-be9tpzzppz4e7r4iipzerrefo 192.168.200.200:2377
    - docker swarm join --token SWMTKN-1-29t9smpqemv4s76r6wgrva06i6my1zjn8nm3ls8lereyfprviy-45dan0frn0g3lmgqxqecsmpzz 192.168.201.200:2377
    
- 매니저에서 토큰 확인: docker swarm join-token worker
- 다중 매니저 노드를 구성하는 경우에는 매니저 노드 추가에 대한 조인 키도 조회가 가능:
    - `docker swarm join-token manager`
- 노드 확인:  `docker node ls`
- 도커 정보를 확인해보면 Swarm 도 확인하는 것이 가능: `docker info`
- 새로 만들어진 네트워크 확인: `docker network ls`
- 운영 도중 노드를 확장하고자 하는 경우는 새로운 토큰이 필요할 수 있는데 이 경우는 2개의 명령으로 토큰 받는 것이 가능
    - `docker swarm join-token --rotate worker`
    - `docker swarm join-token -q worker`
- 노드 연결 중지
    - `docker swarm leave`
- 도커 스웜 시각화 도구
    
    `docker service create --name=viz_swarm -p 7070:8080 --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock dockersmaples/visualizer`
    
- 스웜모드를 GUI로 관리하기 위한 도구
    
    `docker run -it --rm --name swarmpit --volume /var/run/docker.sock:/var/run/docker.socket swarmpit/install`
    

## 서비스 생성 (Swarm-manager)

1. 서비스 생성
    
    `docker service create --name swarm-start alpine:3 /bin/sh -c "while true; do echo 'Docker Swarm mode Start.'; sleep 3; done"`
    
2. 서비스 확인
    
    `docker service ls`
    
3. 컨테이너 정보 확인
    
    `docker service ps [container_name]`
    
4. 로그 확인
    
    `docker service logs -f [container_ID]`
    
5. 서비스 중지
    
    `docker service rm [container_name]`
    

### docker-compose --scale 옵션 과 docker swarm 의 replica 와 차이점

- docker-compose 에서 scale 옵션은 하나의 노드(도커 엔진)에 여러 개의 컨테이너를 생성하는 것이고 docker swarm은 여러 도커 엔진에 여러 개의 컨테이너를 생성하는 것입니다.
- docker-compose 에서 scale 옵션을 이용해서 하나의 이미지로 여러 개의 컨테이너로 만들 때 주의할 점은 포트를 외부로 노출한다면 포트 여러 개를 매핑해야 합니다.
- swarm-manager에서 nginx 서비스를 2개 생성
    
    `docker service create --name web-alb --constraint node.role==worker --replicas 2 --publish 8001:80 nginx`
    
    - 여러 개의 서비스를 만들고자 하는 경우는 replicas 옵션을 이용하는데 이 때 노드의 개수보다 더 많은 서비스 개수를 요청하면 특정 노드에 여러 개가 배치됩니다.
- global을 설정하면 노드가 추가되는 경우 자동으로 확장됩니다.
- replicas를 이용해서 컨테이너를 생성하면 자동 복구 기능을 사용할 수 있습니다.
- service update를 수행하면 롤링 업데이트 수행
    - 서비스를 만들 때 --update-delay 옵션을 이용해서 하나가 업데이트가 되고 난 후 얼마나 대기 할 것인지 설정할 수 있습니다.
    - 서비스를 만들 때 옵션을 지정하지 않으면 1개씩 롤링 업데이트가 수행되지만 --update-parallelism 에 개수를 설정하면 개수만큼 롤링 업데이트가 발생합니다.
    - 실패한 경우 rollback 을 이용해서 이전으로 돌아갈 수 있습니다. 돌아갈 수 있는 개수는 도커 정보를 확인하면 됩니다.
- docker-compose.yml 파일을 가지고 만든 컨테이너는 docker stack deploy --compose 파일경로 서비스이름 으로 배포합니다.

## 컨테이너 오케스트레이션

- 여러 개의 컨테이너를 하나로 묶어서 관리하는 것

### 장점

1. 고가용성
    - 여러 개의 애플리케이션 인스턴스를 실행한 후, 인스턴스가 실패하면 자동으로 다른 인스턴스로 교체 가능함.
2. 규모 확장성

### 오케스트레이션 도구

- Docker Swarm
    - 도커 컨테이너 엔진을 개발한 팀에서 개발
    - 설정, 실행이 간편하지만 유연성이 부족
- Apach Methos
    - 데이터 센터 와 클라우드 환경 모두에서 컴퓨팅, 메모리 및 스토리지를 관리하는 Low Level 오케스트레이션 도구
- Kubernates
    - k8s 라는 이름으로도 불리는 구글이 개발한 오픈소스 컨테이너 오케스트레이션 도구
    - 오픈소스가 된 후에는 엔터프라이즈 환경에서 컨테이너를 실행하고 오케스트레이션하는 사실상의 표준
    - 매우 큰 커뮤니티를 가진 성숙도 높은 제품
    - 아파치 메소스보다 조작이 간단하고 도커 스웜보다는 유연성이 뛰어남

### 쿠버네티스 아키텍쳐

- 노드 유형 : VM, 베어메탈 호스트, 라즈베리 파이 등
    - Master Node : Control Plane Application 실행을 담당
    - Worker Node: 쿠버네티스에 배포되는 애플리케이션 실행을 담당
- Control Plane: 마스터 노드에서 실행되는 애플리케이션과 서비스의 집합
    - 특화된 서비스 존재
    - kube-apisever: 쿠버네티슽에 전송된 커맨드 처리
    - kube-sheduler: 워크 로드 배치할 노드 결정
    - kube-controller-manger:  클러스터에서 실행 중인 애플리케이션 설정
    - etcd: 클러스터 설정을 포함하는 분산 키 - 값 저장소
    - 컴포넌트들은 모든 마스터 노드에서 실행되는 시스템 서비스 형태로 존재
    - 클러스터 생성 라이브러리, 클라우드 벤더가 제공하는 서비스(EKS)를 사용하면 프로덕션 환경에서 자동으로 시작할 수 있음.
- 쿠버네티스 API Server
    - HTTPS(Port - 443) 요청을 받는 컴포넌트
    - 쿠버네티스 API 서버 요청 시, etcd에서 현재 클러스터 정보를 확인

사용자 인증 → 명령어 인증 → API Server  → kubelet → Docker

- etcd
    - API Server는 파드를 만든다는 사실을 etcd에 알리고 사용자에게 파드가 생성되었음을 알리는 기능을 수행
    - etcd는 클러스터의 상태를 저장하는 컴포넌트
    - 클러스터에 필요한 정보, 파드와 같은 리소스들의 정보가 저장되는 곳이 etcd
    - key-value 형태로 저장

- 스케줄러
    - 파드를 위치시킬 워커 노드를 확인하는 컴포넌트
    - 워커 노드가 확인되면 API Server에게 알린 후 etcd에 파드가 생성될 것이라고 저장
    - 파드가 생성되면 kubelet은 API Server에 생성되었다는 사실을 알려주고 그 정보를 etcd에 업데이트

- kubelet
    - API Server로부터 생성될 워커 노드에 있는 kubelet에게 파드 생성 정보를 전달하고 kubelet은 이 정보를 기반으로 파드를 생성
    - 파드가 생성되면 kubelet은 API Server에 생성되었다는 사실을 알려주고 그 정보를 etcd에 업데이트(어떤 워커 노드에 어떤 파드가 생성되었는지 저장)
- Controller Manager
    - 컴포넌트의 상태를 지속적으로 모니터링하는 동시에 실행 상태를 유지하는 역할을 수행
    - 특정 노드 와 통신이 불가능하다고 판단되면 해당 노드에 할당된 파드를 제거하고 다른 워커 노드에서 파드를 생성해 서비스가 계속되도록 합니다.
- Proxy
    - 클러스터의 모든 노드에서 실행되는 네트워크 프록시
    - 노드에 대한 네트워크 규칙을 관리
    - 클러스터 내부 와 외부에 대한 통신을 담당
- Container Runtime
    - 컨테이너 런타임은 컨테이너 실행을 담당
    - 여러 종류의 런타임을 지원하는데 최신 버전에서는 도커는 지원 중단되고 컨테이너디 와 크라이오 등을 사용
- etcd, API Server, Controller Manager, Scheduler 는 Master Node 에 존재하고 Proxy, Kubelet, Container Runtime은 워커 노드에 존재

## 쿠버네티스 컨트롤러

=>파드를 관리하는 역할을 수행하는 객체

=>Deployment

- 쿠버네티스에서 상태가 없는 애플리케이션을 배포할 때 사용하는 가장 기본적인 컨트롤러
- 레플리카 셋의 상위 개념이면서 파드를 배포할 때 사용
- 파드를 배포할 때는 디플로이먼트 나 서비스를 이용
- yaml 파일 기본 구조

apiVersion: apps/v1

kind: Deployment

metadata:

name: 이름

labels:

app: 레이블 설정

spec:

replicas: 레플리카 개수

selector:

metadata: 파드 실행 설정

=>ReplicaSet

- 몇 개의 파드를 유지할 지 결정하는 컨트롤러
- ReplicaSet 과 ReplicaController는 다름
- ReplicaSet은 집합 기반으로 in, not in, exists 같은 연산자를 지원하지만 Replica

https://docs.google.com/document/d/1Uu0Pn7GUJQNymAiw9tWrb1bXAbDO_M_ZkjIc8TgsD-8/edit?tab=t.0